{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory 최적화 이전 버전\n",
    "- 모든 정보를 램에 올려두고 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 효율적인 gpu 사용\n",
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 폴더 내 모든 파일 리스트 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img_idx_stride = 30 # 이미지 번호가 뛰는 단위. 00000 -> 00030 ...\n",
    "look = 0\n",
    "scaler = 3\n",
    "scaled = (1640//scaler + 1, 590//scaler)\n",
    "crop_x = 1\n",
    "crop_y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(target):\n",
    "    '''\n",
    "    # Input\n",
    "    target : 뒤져볼 폴더명\n",
    "    \n",
    "    # Output\n",
    "    imgs, labels = 데이터와 정답. numpy 형식\n",
    "    '''\n",
    "    print('Start to read {} and {}'.format(target+'/data', target+'/label'))\n",
    "    \n",
    "    # Train data Read\n",
    "    imgs = []\n",
    "\n",
    "    ## data Read\n",
    "    for root, dirs, files in os.walk(target+'/data'):\n",
    "        # 일정 순서대로 읽기\n",
    "        dirs.sort()\n",
    "        files.sort()\n",
    "        for fname in files:\n",
    "            full_fname = os.path.join(root, fname)\n",
    "            tmp_img = Image.open(full_fname)\n",
    "            tmp_arr = np.array(tmp_img.resize(scaled))\n",
    "            imgs.append(tmp_arr)\n",
    "            del(tmp_img)\n",
    "            del(tmp_arr)\n",
    "\n",
    "    ##label Read\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(target+'/label'):\n",
    "        # 일정 순서대로 읽기\n",
    "        dirs.sort()\n",
    "        files.sort()\n",
    "        for fname in files:\n",
    "            full_fname = os.path.join(root, fname)\n",
    "            tmp_img = Image.open(full_fname)\n",
    "            tmp_arr = np.array(tmp_img.resize(scaled))[:-crop_y,:-crop_x]\n",
    "            labels.append(tmp_arr)\n",
    "            del(tmp_img)\n",
    "            del(tmp_arr)\n",
    "\n",
    "    X = np.array(imgs)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    del(labels)\n",
    "    del(imgs)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "    # print(\"X_train has {} shape, y_train has {} shape\".format(X_train.shape, y_train.shape))\n",
    "\n",
    "    # Make Global variables\n",
    "    #total, width, height = X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to read train/data and train/label\n",
      "X_train has (17790, 196, 547, 3) shape, y_train has (17790, 194, 546) shape\n",
      "Start to read test/data and test/label\n",
      "X_test has (3357, 196, 547, 3) shape, y_test has (3357, 194, 546) shape\n"
     ]
    }
   ],
   "source": [
    "# Train data Read\n",
    "X_train, y_train = read_set('train')\n",
    "print(\"X_train has {} shape, y_train has {} shape\".format(X_train.shape, y_train.shape))\n",
    "\n",
    "# Test data Read\n",
    "X_test, y_test = read_set('test')\n",
    "print(\"X_test has {} shape, y_test has {} shape\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "# Make Global variables\n",
    "train_size, test_size , width, height = X_train.shape[0], X_test.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "rnum = np.random.randint(X_train.shape[0])\n",
    "\n",
    "plt.figure(figsize=(40,4))\n",
    "plt.title(\"Input\")\n",
    "plt.imshow(X_train[rnum])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(40,4))\n",
    "plt.title(\"Output\")\n",
    "plt.imshow(y_train[rnum],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 가공\n",
    "- LSTM에 맞게, 시간 순서 구조를 부여한다. (데이터 수, 가로, 세로) -> (데이터 수, 현재 보고 있는 데이터들, 가로, 세로)\n",
    "- 한 번에 3개의 데이터를 보고, 하나의 데이터를 생성한다.\n",
    "\n",
    "#### Data 정보\n",
    "- Training set : [CULane](https://xingangpan.github.io/projects/CULane.html) - driver_182_30frame.tar.gz, 5.5GB\n",
    "- Validation set : [CULane](https://xingangpan.github.io/projects/CULane.html) - driver_37_30frame.tar.gz, 1GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_time(data_size, X, y, memory_size = 3):\n",
    "    # Make time-dependent data\n",
    "    # (data_idx, x, y) -> (data_idx, looking, x, y)\n",
    "    X_t = np.zeros((data_size, memory_size, width, height, 3),dtype='uint8')\n",
    "    y_t = np.expand_dims(y, axis = 3)\n",
    "\n",
    "    for i,e in enumerate(X):\n",
    "        try:\n",
    "            X_t[i] = X[i:i+memory_size]\n",
    "        except:\n",
    "            print('* Error : stop at idx {}'.format(i))\n",
    "            break\n",
    "    \n",
    "    return X_t, y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 램 모자란다!!\n",
    "- 램 증가 (연구실에 있는거 동원)\n",
    "- 블럭 단위로 읽어오기 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 기간을 정한다.\n",
    "memory_size = 3\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "X_train_t, y_train_t = give_time(train_size, X_train, y_train, memory_size)\n",
    "del(X_train)\n",
    "del(y_train)\n",
    "\n",
    "X_test_t, y_test_t = give_time(test_size, X_test, y_test, memory_size)\n",
    "del(X_test)\n",
    "del(y_test)\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time-dependency is correct\n",
    "for i in range(memory_size):\n",
    "    plt.figure(figsize=(40,4))\n",
    "    plt.imshow(X_train_t[rnum, i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size, epochs and pool size below are all paramaters to fiddle with for optimization\n",
    "batch_size = 10\n",
    "epochs = 3\n",
    "pool_size = (2, 2)\n",
    "input_shape = X_train_t.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary items from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, UpSampling2D\n",
    "from keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D, ConvLSTM2D, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "\n",
    "### Here is the actual neural network ###\n",
    "model = Sequential()\n",
    "# Normalizes incoming inputs. First layer needs the input shape to work\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "# Below layers were re-named for easier reading of model summary; this not necessary\n",
    "# LSTM Conv Layer 1\n",
    "model.add(ConvLSTM2D(filters=8, kernel_size=(3, 3),strides=(1,1), data_format='channels_last',\n",
    "                     padding='valid', return_sequences=True))\n",
    "\n",
    "# LSTM Conv Layer 2\n",
    "model.add(ConvLSTM2D(filters=16, kernel_size=(3, 3),strides=(1,1), data_format='channels_last',\n",
    "                     padding='valid'))\n",
    "\n",
    "# Pooling 1\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Conv Layer 3\n",
    "model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv3'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv Layer 4\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv4'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv Layer 5\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv5'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Pooling 2\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Conv Layer 6\n",
    "model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv6'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv Layer 7\n",
    "model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv7'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Pooling 3\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Upsample 1\n",
    "model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "# Deconv 1\n",
    "model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv1'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Deconv 2\n",
    "model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv2'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Upsample 2\n",
    "model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "# Deconv 3\n",
    "model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv3'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Deconv 4\n",
    "model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv4'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Deconv 5\n",
    "model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv5'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Upsample 3\n",
    "model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "# Deconv 6\n",
    "model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv6'))\n",
    "\n",
    "# Deconv 7\n",
    "model.add(Conv2DTranspose(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv7'))\n",
    "\n",
    "# Final layer - only including one channel so 1 filter\n",
    "model.add(Conv2DTranspose(1, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and training the model\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "hist = model.fit(x=X_train_t, y=y_train_t, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test_t, y_test_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 및 손실율 추산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## summarize history for accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "## summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#[출처] 케라스(Keras) 기본 - 모델 학습, 히스토리 기능, 모델(신경망) 생성, 시각화 등|작성자 예비개발자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final accuracy is {:.3f}%\".format(hist.history['val_acc'][-1]*100))\n",
    "\n",
    "# Save model architecture and weights\n",
    "model.save('tanuki_network.h5')\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
